{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4117d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some standard imports\n",
    "import io\n",
    "import numpy as np\n",
    "import onnx\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14518540",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        \"\"\"\n",
    "        input_dim (int): size of the input features\n",
    "        output_dim (int): size of the output\n",
    "        \"\"\"\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_dim, 2)\n",
    "        self.fc2 = torch.nn.Linear(2, output_dim)\n",
    "\n",
    "    def forward(self, x): # there are different ways of implementing this\n",
    "        x = self.fc1(x)\n",
    "        x = nn.functional.sigmoid(x)\n",
    "        x = self.fc2(x)\n",
    "        return [x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2898a3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    import numpy as np\n",
    "    # create instance of Net\n",
    "    model = Net(2,1)\n",
    "\n",
    "    #model.state_dict()\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    x = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], device=device).float()\n",
    "    y = torch.tensor([[0], [1], [1], [0]], device=device).view(4,1).float()\n",
    "\n",
    "    # convert numpy array to tensor\n",
    "    x_tensor = torch.clone(x)\n",
    "    y_tensor = torch.clone(y)\n",
    "\n",
    "    # set training mode\n",
    "    model.train() \n",
    "    # In PyTorch, models have a train() method which, somewhat disappointingly, \n",
    "    # does NOT perform a training step. Its only purpose is to set the model to training mode. \n",
    "    # Why is this important? Some models may use mechanisms like Dropout, for instance, \n",
    "    # which have distinct behaviors during training and evaluation phases.\n",
    "\n",
    "\n",
    "    # set training parameters\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "    criterion = torch.nn.MSELoss() # defines a MSE Loss function\n",
    "\n",
    "    # defines number of epochs\n",
    "    num_epochs = 50001\n",
    "    # start to train\n",
    "    epoch_loss = []\n",
    "    for epoch in range(num_epochs):\n",
    "        # forward\n",
    "        outputs = model(x_tensor)[0]\n",
    "\n",
    "        # calculate loss\n",
    "        loss = criterion(outputs, y_tensor)\n",
    "\n",
    "        # update weights\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # save loss of this epoch\n",
    "        epoch_loss.append(loss.data.numpy().tolist()) \n",
    "        \n",
    "        if epoch % 1000 == 0:\n",
    "            print(\"Epoch: {0}, Loss: {1}, \".format(epoch, loss.to(\"cpu\").detach().numpy()))\n",
    "    \n",
    "    print(model.state_dict())\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7442e291",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /opt/conda/conda-bld/pytorch_1607370172916/work/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py:1639: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.4190930128097534, \n",
      "Epoch: 1000, Loss: 0.24957773089408875, \n",
      "Epoch: 2000, Loss: 0.24921050667762756, \n",
      "Epoch: 3000, Loss: 0.24883157014846802, \n",
      "Epoch: 4000, Loss: 0.24838417768478394, \n",
      "Epoch: 5000, Loss: 0.24780577421188354, \n",
      "Epoch: 6000, Loss: 0.24701914191246033, \n",
      "Epoch: 7000, Loss: 0.24592408537864685, \n",
      "Epoch: 8000, Loss: 0.24439266324043274, \n",
      "Epoch: 9000, Loss: 0.24227480590343475, \n",
      "Epoch: 10000, Loss: 0.2394213080406189, \n",
      "Epoch: 11000, Loss: 0.23572468757629395, \n",
      "Epoch: 12000, Loss: 0.23115622997283936, \n",
      "Epoch: 13000, Loss: 0.22576919198036194, \n",
      "Epoch: 14000, Loss: 0.21965807676315308, \n",
      "Epoch: 15000, Loss: 0.21290390193462372, \n",
      "Epoch: 16000, Loss: 0.2055438756942749, \n",
      "Epoch: 17000, Loss: 0.197567880153656, \n",
      "Epoch: 18000, Loss: 0.18890702724456787, \n",
      "Epoch: 19000, Loss: 0.17937445640563965, \n",
      "Epoch: 20000, Loss: 0.16857603192329407, \n",
      "Epoch: 21000, Loss: 0.1558437943458557, \n",
      "Epoch: 22000, Loss: 0.14027494192123413, \n",
      "Epoch: 23000, Loss: 0.12101361155509949, \n",
      "Epoch: 24000, Loss: 0.09792755544185638, \n",
      "Epoch: 25000, Loss: 0.07247088104486465, \n",
      "Epoch: 26000, Loss: 0.0478782095015049, \n",
      "Epoch: 27000, Loss: 0.02782876044511795, \n",
      "Epoch: 28000, Loss: 0.014255543239414692, \n",
      "Epoch: 29000, Loss: 0.006547963246703148, \n",
      "Epoch: 30000, Loss: 0.002765404060482979, \n",
      "Epoch: 31000, Loss: 0.001100729452446103, \n",
      "Epoch: 32000, Loss: 0.0004212216299492866, \n",
      "Epoch: 33000, Loss: 0.00015720422379672527, \n",
      "Epoch: 34000, Loss: 5.7766108511714265e-05, \n",
      "Epoch: 35000, Loss: 2.1033425582572818e-05, \n",
      "Epoch: 36000, Loss: 7.618720701429993e-06, \n",
      "Epoch: 37000, Loss: 2.7531120849744184e-06, \n",
      "Epoch: 38000, Loss: 9.961204341379926e-07, \n",
      "Epoch: 39000, Loss: 3.5902547779187444e-07, \n",
      "Epoch: 40000, Loss: 1.309614674482873e-07, \n",
      "Epoch: 41000, Loss: 4.6525556740562024e-08, \n",
      "Epoch: 42000, Loss: 1.8396050904812e-08, \n",
      "Epoch: 43000, Loss: 6.525052498318473e-09, \n",
      "Epoch: 44000, Loss: 3.651798863302247e-09, \n",
      "Epoch: 45000, Loss: 1.969300278403807e-09, \n",
      "Epoch: 46000, Loss: 1.8787709166190325e-09, \n",
      "Epoch: 47000, Loss: 1.8787709166190325e-09, \n",
      "Epoch: 48000, Loss: 1.8787709166190325e-09, \n",
      "Epoch: 49000, Loss: 1.8787709166190325e-09, \n",
      "Epoch: 50000, Loss: 1.8787709166190325e-09, \n",
      "OrderedDict([('fc1.weight', tensor([[1.4154, 1.4126],\n",
      "        [3.6232, 3.6075]])), ('fc1.bias', tensor([-1.8672, -0.6550])), ('fc2.weight', tensor([[-3.4276,  3.0762]])), ('fc2.bias', tensor([-0.5928]))])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\": \n",
    "\n",
    "    # Let's build our model \n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cba28099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 1.]])\n",
      "Model has been converted to ONNX\n"
     ]
    }
   ],
   "source": [
    "# PYTORCH FINN-ONNX EXPORT\n",
    "import torch.onnx \n",
    "# set the model to inference mode \n",
    "model = Net(2,1)\n",
    "#model.state_dict()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "x = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], device=device).float()\n",
    "print(x)\n",
    "y = torch.tensor([[0], [1], [1], [0]], device=device).view(4,1).float()\n",
    "model.eval() \n",
    "\n",
    "# Let's create a dummy input tensor  \n",
    "dummy_input = x # no test input\n",
    "\n",
    "# Export the model   \n",
    "torch.onnx.export(model,         # model being run \n",
    "                  dummy_input,       # model input (or a tuple for multiple inputs) \n",
    "                 \"xor_network.onnx\",       # where to save the model  \n",
    "                 export_params=True,  # store the trained parameter weights inside the model file \n",
    "                 opset_version=10,    # the ONNX version to export the model to \n",
    "                 do_constant_folding=True,  # whether to execute constant folding for optimization \n",
    "                 input_names = ['modelInput'],   # the model's input names \n",
    "                 output_names = ['modelOutput'], # the model's output names \n",
    "                 dynamic_axes={'modelInput' : {0 : 'batch_size'},    # variable length axes \n",
    "                                'modelOutput' : {0: 'batch_size'}}) \n",
    "\n",
    "print('Model has been converted to ONNX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f26887b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the ONNX model with ONNXâ€™s API\n",
    "\n",
    "onnx_model = onnx.load(\"xor_network.onnx\")\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6591ab0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving 'xor_network.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7feee6f72bb0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from finn.util.visualization import showSrc, showInNetron\n",
    "from finn.util.basic import make_build_dir\n",
    "\n",
    "    \n",
    "showInNetron(\"xor_network.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ad83b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.core.modelwrapper import ModelWrapper\n",
    "model = ModelWrapper(\"xor_network.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60805db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor name: modelInput\n",
      "Output tensor name: modelOutput\n",
      "Input tensor shape: [0, 2]\n",
      "Output tensor shape: [0, 1]\n",
      "Input tensor datatype: FLOAT32\n",
      "Output tensor datatype: FLOAT32\n",
      "List of node operator types in the graph: \n",
      "['Gemm', 'Sigmoid', 'Gemm']\n"
     ]
    }
   ],
   "source": [
    "from finn.core.datatype import DataType\n",
    "\n",
    "finnonnx_in_tensor_name = model.graph.input[0].name\n",
    "finnonnx_out_tensor_name = model.graph.output[0].name\n",
    "print(\"Input tensor name: %s\" % finnonnx_in_tensor_name)\n",
    "print(\"Output tensor name: %s\" % finnonnx_out_tensor_name)\n",
    "finnonnx_model_in_shape = model.get_tensor_shape(finnonnx_in_tensor_name)\n",
    "finnonnx_model_out_shape = model.get_tensor_shape(finnonnx_out_tensor_name)\n",
    "print(\"Input tensor shape: %s\" % str(finnonnx_model_in_shape))\n",
    "print(\"Output tensor shape: %s\" % str(finnonnx_model_out_shape))\n",
    "finnonnx_model_in_dt = model.get_tensor_datatype(finnonnx_in_tensor_name)\n",
    "finnonnx_model_out_dt = model.get_tensor_datatype(finnonnx_out_tensor_name)\n",
    "print(\"Input tensor datatype: %s\" % str(finnonnx_model_in_dt.name))\n",
    "print(\"Output tensor datatype: %s\" % str(finnonnx_model_out_dt.name))\n",
    "print(\"List of node operator types in the graph: \")\n",
    "print([x.op_type for x in model.graph.node])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21aeb1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Preparation\n",
    "\n",
    "#Tidy up transformations\n",
    "from finn.transformation.general import GiveReadableTensorNames, GiveUniqueNodeNames, RemoveStaticGraphInputs\n",
    "from finn.transformation.infer_shapes import InferShapes\n",
    "from finn.transformation.infer_datatypes import InferDataTypes\n",
    "from finn.transformation.fold_constants import FoldConstants\n",
    "\n",
    "model = model.transform(InferShapes())\n",
    "model = model.transform(FoldConstants())\n",
    "model = model.transform(GiveUniqueNodeNames())\n",
    "model = model.transform(GiveReadableTensorNames())\n",
    "model = model.transform(InferDataTypes())\n",
    "model = model.transform(RemoveStaticGraphInputs())\n",
    "\n",
    "model.save(\"xor_network.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "105585a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving 'xor_network.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7feee6f7c100>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(\"xor_network.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ff6036b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/finn-base/src/finn/transformation/infer_data_layouts.py:124: UserWarning: Assuming 2D input is NC\n",
      "  warnings.warn(\"Assuming 2D input is NC\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving 'xor_network_with_prepoc.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7feee7006760>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding Pre and Postprocessing\n",
    "from finn.util.pytorch import ToTensor\n",
    "from finn.transformation.merge_onnx_models import MergeONNXModels\n",
    "from finn.core.datatype import DataType\n",
    "import brevitas.onnx as bo\n",
    "\n",
    "model = ModelWrapper(\"xor_network.onnx\")\n",
    "global_inp_name = model.graph.input[0].name\n",
    "ishape = model.get_tensor_shape(global_inp_name)\n",
    "# preprocessing: torchvision's ToTensor divides uint8 inputs by 255\n",
    "totensor_pyt = ToTensor()\n",
    "chkpt_preproc_name = \"xor_network_prepoc.onnx\"\n",
    "bo.export_finn_onnx(totensor_pyt, ishape, chkpt_preproc_name)\n",
    "\n",
    "# join preprocessing and core model\n",
    "pre_model = ModelWrapper(chkpt_preproc_name)\n",
    "model = model.transform(MergeONNXModels(pre_model))\n",
    "# add input quantization annotation: UINT8 for all BNN-PYNQ models\n",
    "global_inp_name = model.graph.input[0].name\n",
    "model.set_tensor_datatype(global_inp_name, DataType[\"UINT8\"])\n",
    "\n",
    "model.save(\"xor_network_with_prepoc.onnx\")\n",
    "showInNetron(\"xor_network_with_prepoc.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8a1eff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving 'xor_network_prepost.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7feee7006a90>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from finn.transformation.insert_topk import InsertTopK\n",
    "\n",
    "# postprocessing: insert Top-1 node at the end\n",
    "model = model.transform(InsertTopK(k=1))\n",
    "chkpt_name = \"xor_network_prepost.onnx\"\n",
    "# tidy-up again\n",
    "model = model.transform(InferShapes())\n",
    "model = model.transform(FoldConstants())\n",
    "model = model.transform(GiveUniqueNodeNames())\n",
    "model = model.transform(GiveReadableTensorNames())\n",
    "model = model.transform(InferDataTypes())\n",
    "model = model.transform(RemoveStaticGraphInputs())\n",
    "model.save(chkpt_name)\n",
    "\n",
    "showInNetron(\"xor_network_prepost.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75a2098c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class Streamline(Transformation):\n",
      "    \"\"\"Apply the streamlining transform, see arXiv:1709.04060.\"\"\"\n",
      "\n",
      "    def apply(self, model):\n",
      "        streamline_transformations = [\n",
      "            ConvertSubToAdd(),\n",
      "            ConvertDivToMul(),\n",
      "            BatchNormToAffine(),\n",
      "            ConvertSignToThres(),\n",
      "            MoveMulPastMaxPool(),\n",
      "            MoveScalarLinearPastInvariants(),\n",
      "            AbsorbSignBiasIntoMultiThreshold(),\n",
      "            MoveAddPastMul(),\n",
      "            MoveScalarAddPastMatMul(),\n",
      "            MoveAddPastConv(),\n",
      "            MoveScalarMulPastMatMul(),\n",
      "            MoveScalarMulPastConv(),\n",
      "            MoveAddPastMul(),\n",
      "            CollapseRepeatedAdd(),\n",
      "            CollapseRepeatedMul(),\n",
      "            MoveMulPastMaxPool(),\n",
      "            AbsorbAddIntoMultiThreshold(),\n",
      "            FactorOutMulSignMagnitude(),\n",
      "            AbsorbMulIntoMultiThreshold(),\n",
      "            Absorb1BitMulIntoMatMul(),\n",
      "            Absorb1BitMulIntoConv(),\n",
      "            RoundAndClipThresholds(),\n",
      "        ]\n",
      "        for trn in streamline_transformations:\n",
      "            model = model.transform(trn)\n",
      "            model = model.transform(RemoveIdentityOps())\n",
      "            model = model.transform(GiveUniqueNodeNames())\n",
      "            model = model.transform(GiveReadableTensorNames())\n",
      "            model = model.transform(InferDataTypes())\n",
      "        return (model, False)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Streamlining\n",
    "from finn.transformation.streamline import Streamline\n",
    "showSrc(Streamline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9956be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving 'xor_network_streamlined.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7feee0fd9970>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from finn.transformation.streamline.reorder import MoveScalarLinearPastInvariants\n",
    "import finn.transformation.streamline.absorb as absorb\n",
    "\n",
    "model = ModelWrapper(\"xor_network_prepost.onnx\")\n",
    "# move initial Mul (from preproc) past the Reshape\n",
    "model = model.transform(MoveScalarLinearPastInvariants())\n",
    "# streamline\n",
    "model = model.transform(Streamline())\n",
    "model.save(\"xor_network_streamlined.onnx\")\n",
    "showInNetron(\"xor_network_streamlined.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bea951fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving 'xor_network_rdy_for_hls.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7feec853bc70>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from finn.transformation.bipolar_to_xnor import ConvertBipolarMatMulToXnorPopcount\n",
    "from finn.transformation.streamline.round_thresholds import RoundAndClipThresholds\n",
    "from finn.transformation.infer_data_layouts import InferDataLayouts\n",
    "from finn.transformation.general import RemoveUnusedTensors\n",
    "\n",
    "model = model.transform(ConvertBipolarMatMulToXnorPopcount())\n",
    "model = model.transform(absorb.AbsorbAddIntoMultiThreshold())\n",
    "model = model.transform(absorb.AbsorbMulIntoMultiThreshold())\n",
    "# absorb final add-mul nodes into TopK\n",
    "model = model.transform(absorb.AbsorbScalarMulAddIntoTopK())\n",
    "model = model.transform(RoundAndClipThresholds())\n",
    "\n",
    "# bit of tidy-up\n",
    "model = model.transform(InferDataLayouts())\n",
    "model = model.transform(RemoveUnusedTensors())\n",
    "\n",
    "model.save(\"xor_network_rdy_for_hls.onnx\")\n",
    "showInNetron(\"xor_network_rdy_for_hls.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5c52c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving 'xor_network_hls_layers.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7feec85819a0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import finn.transformation.fpgadataflow.convert_to_hls_layers as to_hls\n",
    "model = ModelWrapper(\"xor_network_rdy_for_hls.onnx\")\n",
    "# Mul \n",
    "model = model.transform(to_hls.InferLookupLayer())\n",
    "# TopK to LabelSelect\n",
    "model = model.transform(to_hls.InferLabelSelectLayer())\n",
    "model.save(\"xor_network_hls_layers.onnx\")\n",
    "showInNetron(\"xor_network_hls_layers.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19fe9b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving 'xor_network_dataflow_parent.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f0d4ca0ee50>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dataflow Partition\n",
    "\n",
    "from finn.transformation.fpgadataflow.create_dataflow_partition import CreateDataflowPartition\n",
    "\n",
    "model = ModelWrapper(\"xor_network_hls_layers.onnx\")\n",
    "parent_model = model.transform(CreateDataflowPartition())\n",
    "parent_model.save(\"xor_network_dataflow_parent.onnx\")\n",
    "showInNetron(\"xor_network_dataflow_parent.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "829980e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since Dataflow partition node is not created, something might be wrong. \n",
    "# I will still continue.\n",
    "# from finn.custom_op.registry import getCustomOp\n",
    "# sdp_node = parent_model.get_nodes_by_op_type(\"StreamingDataflowPartition\")[0]\n",
    "# sdp_node = getCustomOp(sdp_node)\n",
    "# dataflow_model_filename = sdp_node.get_nodeattr(\"model\")\n",
    "# showInNetron(dataflow_model_filename)\n",
    "\n",
    "model = ModelWrapper(\"xor_network_dataflow_parent.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e47f61b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Empty module name",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-fe99f2d20ade>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfc0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfc0w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetCustomOp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfc0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CustomOp wrapper is of class \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfc0w\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/finn-base/src/finn/custom_op/registry.py\u001b[0m in \u001b[0;36mgetCustomOp\u001b[0;34m(node)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mdomain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mopset_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         assert type(opset_module.custom_op) is dict, (\n\u001b[1;32m     39\u001b[0m             \u001b[0;34m\"custom_op dict not found in Python module %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdomain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_sanity_check\u001b[0;34m(name, package, level)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Empty module name"
     ]
    }
   ],
   "source": [
    "fc0 = model.graph.node[0]\n",
    "fc0w = getCustomOp(fc0)\n",
    "\n",
    "print(\"CustomOp wrapper is of class \" + fc0w.__class__.__name__)\n",
    "\n",
    "fc0w.get_nodeattr_types()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1949937c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
