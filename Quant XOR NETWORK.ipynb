{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d6090ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from brevitas.quant.base import UintQuant, MaxStatsScaling\n",
    "from brevitas.quant.solver.weight import WeightQuantSolver\n",
    "from brevitas.quant.solver.bias import BiasQuantSolver\n",
    "from brevitas.quant.solver.act import ActQuantSolver\n",
    "from brevitas.core.function_wrapper.ops_ste import CeilSte\n",
    "from brevitas.inject.enum import RestrictValueType\n",
    "\n",
    "class Uint16Bias(UintQuant, MaxStatsScaling, BiasQuantSolver):\n",
    "    scaling_per_output_channel = False\n",
    "    restrict_scaling_type = RestrictValueType.POWER_OF_TWO\n",
    "    bit_width = 16\n",
    "    restrict_value_float_to_int_impl = CeilSte\n",
    "    requires_input_bit_width = False\n",
    "    requires_input_scale = True\n",
    "    \n",
    "\n",
    "class Uint2Weight(UintQuant, MaxStatsScaling, WeightQuantSolver):\n",
    "    scaling_per_output_channel = False\n",
    "    restrict_scaling_type = RestrictValueType.POWER_OF_TWO\n",
    "    bit_width = 2\n",
    "    restrict_value_float_to_int_impl = CeilSte\n",
    "    requires_input_bit_width = False\n",
    "    requires_input_scale = True\n",
    "\n",
    "class Uint2Act(UintQuant, MaxStatsScaling, ActQuantSolver):\n",
    "    scaling_per_output_channel = False\n",
    "    restrict_scaling_type = RestrictValueType.POWER_OF_TWO\n",
    "    bit_width = 2\n",
    "    restrict_value_float_to_int_impl = CeilSte\n",
    "    requires_input_bit_with = False\n",
    "    requires_input_scale = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4117d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some standard imports\n",
    "import io\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from brevitas.nn import QuantLinear, QuantReLU, QuantSigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14518540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass Net(nn.Module):\\n\\n    def __init__(self, input_dim, output_dim):\\n        \\n#        input_dim (int): size of the input features\\n#        output_dim (int): size of the output\\n        \\n        super(Net, self).__init__()\\n        self.fc1 = QuantLinear(input_dim, 2, bias=True,weight_bit_width=2)\\n        self.fc2 = QuantLinear(2, output_dim, bias=True,weight_bit_width=2)\\n\\n    def forward(self, x): # there are different ways of implementing this\\n        x = self.fc1(x)\\n        x = QuantSigmoid(x, act_quant=Uint2Act)\\n        x = self.fc2(x)\\n        return [x]\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        \n",
    "#        input_dim (int): size of the input features\n",
    "#        output_dim (int): size of the output\n",
    "        \n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = QuantLinear(input_dim, 2, bias=True,weight_bit_width=2)\n",
    "        self.fc2 = QuantLinear(2, output_dim, bias=True,weight_bit_width=2)\n",
    "\n",
    "    def forward(self, x): # there are different ways of implementing this\n",
    "        x = self.fc1(x)\n",
    "        x = QuantSigmoid(x, act_quant=Uint2Act)\n",
    "        x = self.fc2(x)\n",
    "        return [x]\n",
    "\"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "608438f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "    input_dim=2\n",
    "    output_dim=1\n",
    "\n",
    "    model = torch.nn.Sequential(\n",
    "            QuantLinear(input_dim, 2, bias=True,weight_bit_width=2),\n",
    "            QuantSigmoid(act_quant=Uint2Act),\n",
    "            QuantLinear(2, output_dim, bias=True,weight_bit_width=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2898a3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    import numpy as np\n",
    "    # create instance of Net\n",
    "#    model = Net(2,1)\n",
    "\n",
    "    #model.state_dict()\n",
    "\n",
    "    input_dim=2\n",
    "    output_dim=1\n",
    "\n",
    "    model = torch.nn.Sequential(\n",
    "            QuantLinear(input_dim, 2, bias=True,weight_bit_width=2),\n",
    "            QuantSigmoid(act_quant=Uint2Act),\n",
    "            QuantLinear(2, output_dim, bias=True,weight_bit_width=2))\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    x = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], device=device).float()\n",
    "    y = torch.tensor([[0], [1], [1], [0]], device=device).view(4,1).float()\n",
    "\n",
    "    # convert numpy array to tensor\n",
    "    x_tensor = torch.clone(x)\n",
    "    y_tensor = torch.clone(y)\n",
    "\n",
    "    # set training mode\n",
    "    model.train() \n",
    "    # In PyTorch, models have a train() method which, somewhat disappointingly, \n",
    "    # does NOT perform a training step. Its only purpose is to set the model to training mode. \n",
    "    # Why is this important? Some models may use mechanisms like Dropout, for instance, \n",
    "    # which have distinct behaviors during training and evaluation phases.\n",
    "\n",
    "\n",
    "    # set training parameters\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "    criterion = torch.nn.MSELoss() # defines a MSE Loss function\n",
    "\n",
    "    # defines number of epochs\n",
    "    num_epochs = 50001\n",
    "    # start to train\n",
    "    epoch_loss = []\n",
    "    for epoch in range(num_epochs):\n",
    "        # forward\n",
    "        outputs = model(x_tensor)[0]\n",
    "\n",
    "        # calculate loss\n",
    "        loss = criterion(outputs, y_tensor)\n",
    "\n",
    "        # update weights\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # save loss of this epoch\n",
    "        epoch_loss.append(loss.data.numpy().tolist()) \n",
    "        \n",
    "        if epoch % 1000 == 0:\n",
    "            print(\"Epoch: {0}, Loss: {1}, \".format(epoch, loss.to(\"cpu\").detach().numpy()))\n",
    "    \n",
    "    print(model.state_dict())\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7442e291",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /opt/conda/conda-bld/pytorch_1607370172916/work/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/loss.py:446: UserWarning: Using a target size (torch.Size([4, 1])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.5281404256820679, \n",
      "Epoch: 1000, Loss: 0.25, \n",
      "Epoch: 2000, Loss: 0.25, \n",
      "Epoch: 3000, Loss: 0.25, \n",
      "Epoch: 4000, Loss: 0.25, \n",
      "Epoch: 5000, Loss: 0.25, \n",
      "Epoch: 6000, Loss: 0.25, \n",
      "Epoch: 7000, Loss: 0.25, \n",
      "Epoch: 8000, Loss: 0.25, \n",
      "Epoch: 9000, Loss: 0.25, \n",
      "Epoch: 10000, Loss: 0.25, \n",
      "Epoch: 11000, Loss: 0.25, \n",
      "Epoch: 12000, Loss: 0.25, \n",
      "Epoch: 13000, Loss: 0.25, \n",
      "Epoch: 14000, Loss: 0.25, \n",
      "Epoch: 15000, Loss: 0.25, \n",
      "Epoch: 16000, Loss: 0.25, \n",
      "Epoch: 17000, Loss: 0.25, \n",
      "Epoch: 18000, Loss: 0.25, \n",
      "Epoch: 19000, Loss: 0.25, \n",
      "Epoch: 20000, Loss: 0.25, \n",
      "Epoch: 21000, Loss: 0.25, \n",
      "Epoch: 22000, Loss: 0.25, \n",
      "Epoch: 23000, Loss: 0.25, \n",
      "Epoch: 24000, Loss: 0.25, \n",
      "Epoch: 25000, Loss: 0.25, \n",
      "Epoch: 26000, Loss: 0.25, \n",
      "Epoch: 27000, Loss: 0.25, \n",
      "Epoch: 28000, Loss: 0.25, \n",
      "Epoch: 29000, Loss: 0.25, \n",
      "Epoch: 30000, Loss: 0.25, \n",
      "Epoch: 31000, Loss: 0.25, \n",
      "Epoch: 32000, Loss: 0.25, \n",
      "Epoch: 33000, Loss: 0.25, \n",
      "Epoch: 34000, Loss: 0.25, \n",
      "Epoch: 35000, Loss: 0.25, \n",
      "Epoch: 36000, Loss: 0.25, \n",
      "Epoch: 37000, Loss: 0.25, \n",
      "Epoch: 38000, Loss: 0.25, \n",
      "Epoch: 39000, Loss: 0.25, \n",
      "Epoch: 40000, Loss: 0.25, \n",
      "Epoch: 41000, Loss: 0.25, \n",
      "Epoch: 42000, Loss: 0.25, \n",
      "Epoch: 43000, Loss: 0.25, \n",
      "Epoch: 44000, Loss: 0.25, \n",
      "Epoch: 45000, Loss: 0.25, \n",
      "Epoch: 46000, Loss: 0.25, \n",
      "Epoch: 47000, Loss: 0.25, \n",
      "Epoch: 48000, Loss: 0.25, \n",
      "Epoch: 49000, Loss: 0.25, \n",
      "Epoch: 50000, Loss: 0.25, \n",
      "OrderedDict([('0.weight', tensor([[ 0.6141, -0.2626],\n",
      "        [-0.0592,  0.5823]])), ('0.bias', tensor([ 0.6827, -0.4594])), ('1.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.runtime_stats.running_stats', tensor(0.7853)), ('2.weight', tensor([[0.1636, 0.3909]])), ('2.bias', tensor([0.3045]))])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\": \n",
    "\n",
    "    # Let's build our model \n",
    "    main()\n",
    "    \n",
    "    # loss is stuck at 0.25 after the first 1000 epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e1d860e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# create a QuantTensor instance to mark input as bipolar during export\\ninput_a = np.random.randint(0, 1, size=input_shape).astype(np.float32)\\ninput_a = 2 * input_a - 1\\nscale = 1.0\\ninput_t = torch.from_numpy(input_a * scale)\\ninput_qt = QuantTensor(\\n    input_t, scale=torch.tensor(scale), bit_width=torch.tensor(1.0), signed=True\\n)\\n\\nbo.export_finn_onnx(\\n    model_for_export, export_path=ready_model_filename, input_t=input_qt\\n)\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import brevitas.onnx as bo\n",
    "from brevitas.quant_tensor import QuantTensor\n",
    "\"\"\"\n",
    "# create a QuantTensor instance to mark input as bipolar during export\n",
    "input_a = np.random.randint(0, 1, size=input_shape).astype(np.float32)\n",
    "input_a = 2 * input_a - 1\n",
    "scale = 1.0\n",
    "input_t = torch.from_numpy(input_a * scale)\n",
    "input_qt = QuantTensor(\n",
    "    input_t, scale=torch.tensor(scale), bit_width=torch.tensor(1.0), signed=True\n",
    ")\n",
    "\n",
    "bo.export_finn_onnx(\n",
    "    model_for_export, export_path=ready_model_filename, input_t=input_qt\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cba28099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/brevitas/src/brevitas/quant_tensor/__init__.py:98: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  training = torch.tensor(training, dtype=torch.bool)\n",
      "/workspace/brevitas/src/brevitas/quant_tensor/__init__.py:96: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  signed = torch.tensor(signed, dtype=torch.bool)\n"
     ]
    }
   ],
   "source": [
    "# PYTORCH FINN-ONNX EXPORT\n",
    "import torch.onnx \n",
    "# set the model to inference mode \n",
    "#model = Net(2,1)\n",
    "#model.state_dict()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "x = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], device=device).float()\n",
    "y = torch.tensor([[0], [1], [1], [0]], device=device).view(4,1).float()\n",
    "model.eval() \n",
    "\n",
    "# Let's create a dummy input tensor  \n",
    "dummy_input = (x) # no test input\n",
    "#dummy_input = QuantTensor(dummy_input,scale=1, bit_width=torch.tensor(1.0), signed=True)\n",
    "\n",
    "# Export the model   \n",
    "\n",
    "torch.onnx.export(model,         # model being run \n",
    "                  dummy_input,       # model input (or a tuple for multiple inputs) \n",
    "                 \"xor_network.onnx\",       # where to save the model  \n",
    "                 export_params=True,  # store the trained parameter weights inside the model file \n",
    "                 opset_version=11,    # the ONNX version to export the model to \n",
    "                 do_constant_folding=True,  # whether to execute constant folding for optimization \n",
    "                 input_names = ['modelInput'],   # the model's input names \n",
    "                 output_names = ['modelOutput'], # the model's output names \n",
    "                 dynamic_axes={'modelInput' : {0 : 'batch_size'},    # variable length axes \n",
    "                                'modelOutput' : {0 : 'batch_size'}}) \n",
    "\"\"\"\n",
    "bo.export_finn_onnx(model,export_path=\"xor_network_q.onnx\", input_t=dummy_input,export_params=True,\n",
    "                   opset_version=10,do_constant_folding=True,input_names=['modelInput'],output_names=['modelOutput'],\n",
    "                    dynamic_axes={'modelInput' : {0 : 'batch_size'},    # variable length axes \n",
    "                                'modelOutput' : {0 : 'batch_size'}}) \n",
    "\"\"\"                    \n",
    "print(\" \") \n",
    "print('Model has been converted to ONNX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f26887b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the ONNX model with ONNX’s API\n",
    "import onnx\n",
    "\n",
    "onnx_model = onnx.load(\"xor_network_q.onnx\")\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6591ab0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving 'xor_network_q.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fd7d82fcd90>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from finn.util.visualization import showSrc, showInNetron\n",
    "from finn.util.basic import make_build_dir\n",
    "\n",
    "    \n",
    "showInNetron(\"xor_network_q.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ad83b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.core.modelwrapper import ModelWrapper\n",
    "model2 = ModelWrapper(\"xor_network_q.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21aeb1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Preparation\n",
    "\n",
    "#Tidy up transformations\n",
    "from finn.transformation.general import GiveReadableTensorNames, GiveUniqueNodeNames, RemoveStaticGraphInputs\n",
    "from finn.transformation.infer_shapes import InferShapes\n",
    "from finn.transformation.infer_datatypes import InferDataTypes\n",
    "from finn.transformation.fold_constants import FoldConstants\n",
    "\n",
    "model2 = model2.transform(InferShapes())\n",
    "model2 = model2.transform(FoldConstants())\n",
    "model2 = model2.transform(GiveUniqueNodeNames())\n",
    "model2 = model2.transform(GiveReadableTensorNames())\n",
    "model2 = model2.transform(InferDataTypes())\n",
    "model2 = model2.transform(RemoveStaticGraphInputs())\n",
    "\n",
    "model2.save(\"xor_network_q.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "105585a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving 'xor_network_q.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fd7d81375e0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(\"xor_network_q.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ff6036b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/finn-base/src/finn/transformation/infer_data_layouts.py:124: UserWarning: Assuming 2D input is NC\n",
      "  warnings.warn(\"Assuming 2D input is NC\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving 'xor_network_q.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fd7d8137190>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding Pre and Postprocessing\n",
    "from finn.util.pytorch import ToTensor\n",
    "from finn.transformation.merge_onnx_models import MergeONNXModels\n",
    "from finn.core.datatype import DataType\n",
    "import brevitas.onnx as bo\n",
    "\n",
    "model2 = ModelWrapper(\"xor_network_q.onnx\")\n",
    "global_inp_name = model2.graph.input[0].name\n",
    "ishape = model2.get_tensor_shape(global_inp_name)\n",
    "# preprocessing: torchvision's ToTensor divides uint8 inputs by 255\n",
    "totensor_pyt = ToTensor()\n",
    "chkpt_preproc_name = \"xor_network_q.onnx\"\n",
    "bo.export_finn_onnx(totensor_pyt, ishape, chkpt_preproc_name)\n",
    "\n",
    "# join preprocessing and core model\n",
    "pre_model = ModelWrapper(chkpt_preproc_name)\n",
    "model2 = model2.transform(MergeONNXModels(pre_model))\n",
    "# add input quantization annotation: UINT8 for all BNN-PYNQ models\n",
    "global_inp_name = model2.graph.input[0].name\n",
    "model2.set_tensor_datatype(global_inp_name, DataType[\"UINT8\"])\n",
    "\n",
    "model2.save(\"xor_network_q.onnx\")\n",
    "showInNetron(\"xor_network_q.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8a1eff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving 'xor_network_q.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fd7d80ec0d0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from finn.transformation.insert_topk import InsertTopK\n",
    "\n",
    "# postprocessing: insert Top-1 node at the end\n",
    "model2 = model2.transform(InsertTopK(k=1))\n",
    "chkpt_name = \"xor_network_q.onnx\"\n",
    "# tidy-up again\n",
    "model2 = model2.transform(InferShapes())\n",
    "model2 = model2.transform(FoldConstants())\n",
    "model2 = model2.transform(GiveUniqueNodeNames())\n",
    "model2 = model2.transform(GiveReadableTensorNames())\n",
    "model2 = model2.transform(InferDataTypes())\n",
    "model2 = model2.transform(RemoveStaticGraphInputs())\n",
    "model2.save(chkpt_name)\n",
    "\n",
    "showInNetron(\"xor_network_q.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75a2098c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class Streamline(Transformation):\n",
      "    \"\"\"Apply the streamlining transform, see arXiv:1709.04060.\"\"\"\n",
      "\n",
      "    def apply(self, model):\n",
      "        streamline_transformations = [\n",
      "            ConvertSubToAdd(),\n",
      "            ConvertDivToMul(),\n",
      "            BatchNormToAffine(),\n",
      "            ConvertSignToThres(),\n",
      "            MoveMulPastMaxPool(),\n",
      "            MoveScalarLinearPastInvariants(),\n",
      "            AbsorbSignBiasIntoMultiThreshold(),\n",
      "            MoveAddPastMul(),\n",
      "            MoveScalarAddPastMatMul(),\n",
      "            MoveAddPastConv(),\n",
      "            MoveScalarMulPastMatMul(),\n",
      "            MoveScalarMulPastConv(),\n",
      "            MoveAddPastMul(),\n",
      "            CollapseRepeatedAdd(),\n",
      "            CollapseRepeatedMul(),\n",
      "            MoveMulPastMaxPool(),\n",
      "            AbsorbAddIntoMultiThreshold(),\n",
      "            FactorOutMulSignMagnitude(),\n",
      "            AbsorbMulIntoMultiThreshold(),\n",
      "            Absorb1BitMulIntoMatMul(),\n",
      "            Absorb1BitMulIntoConv(),\n",
      "            RoundAndClipThresholds(),\n",
      "        ]\n",
      "        for trn in streamline_transformations:\n",
      "            model = model.transform(trn)\n",
      "            model = model.transform(RemoveIdentityOps())\n",
      "            model = model.transform(GiveUniqueNodeNames())\n",
      "            model = model.transform(GiveReadableTensorNames())\n",
      "            model = model.transform(InferDataTypes())\n",
      "        return (model, False)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Streamlining\n",
    "from finn.transformation.streamline import Streamline\n",
    "showSrc(Streamline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9956be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving 'xor_network_q.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fd7d8103820>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from finn.transformation.streamline.reorder import MoveScalarLinearPastInvariants\n",
    "import finn.transformation.streamline.absorb as absorb\n",
    "\n",
    "model = ModelWrapper(\"xor_network_q.onnx\")\n",
    "# move initial Mul (from preproc) past the Reshape\n",
    "model = model.transform(MoveScalarLinearPastInvariants())\n",
    "# streamline\n",
    "model = model.transform(Streamline())\n",
    "model.save(\"xor_network_q.onnx\")\n",
    "showInNetron(\"xor_network_q.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea951fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
